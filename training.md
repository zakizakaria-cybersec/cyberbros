Program Agenda:

**Module 1 : Setting Up Self-Hosted LLMs for Cybersecurity**

```
 Introduction to AI in Cybersecurity (Defensive & Offensive Applications)
 Overview of LLMs (Local vs. Cloud-based)
 Setting up a Self-Hosted LLM (using models like LLaMA)
 Hardware & System Requirements
 Model Selection and Installation (ollama, LM Studio, or TextGen WebUI)
 Privacy & Security Considerations with Local AI
 Integrating LLM with Cybersecurity Tooling (APIs, CLI usage, sandbox environments)
```
```
 Hands-on Labs:
 Deploying and running a local LLM instance
 Querying your LLM for basic cybersecurity tasks
 Configuring secure access and sandbox environments
```
**Module 2 : AI for Threat Hunting and Defensive Security**

```
 Threat Hunting Concepts & Methodologies
 Using AI for:
 Log Analysis & Anomaly Detection
 LLM-assisted Threat Intelligence Correlation
 Behavior Profiling and Pattern Recognition
 Automating Alerts and Response Workflows using LLM
```
```
 Hands-on Labs:
 Feeding log data to LLM and asking threat hunting queries
 Detecting malicious user behavior via AI-assisted anomaly spotting
 Building an automated threat response bot using LLM + SOAR-like scripts
```
**Module 3 : Offensive AI – Using LLMs for Recon, Exploitation & Secure Coding**

```
 Reconnaissance Automation with LLMs:
 Domain analysis, subdomain enumeration, service fingerprinting
 Exploit Generation & Payload Customization
 Prompt engineering for exploit scripts (XSS, SQLi, RCE)
 AI-assisted fuzzing techniques
```
```
 Secure Coding with AI
 Real-time secure code suggestions
 Reviewing and refactoring vulnerable code using LLMs
 Hands-on Labs:
 Generating and modifying PoCs using LLM
 Automating recon tasks with LLM prompts
 Secure code review exercises with vulnerable apps and LLM feedback
```
**Module 4 : AI Red Teaming and Hacking AI-Powered Applications**

```
 Introduction to AI Red Teaming
 Hacking AI-Integrated Applications
 Prompt Injection Attacks
 Direct prompt injection
 Indirect prompt injection (via user inputs or third-party data)
```
```
 LLM Jailbreaking Techniques
 Instruction following overrides
 Adversarial Input Crafting
 Evaluating and Exploiting AI Security Flaws in:
 Chatbots and support agents
 Code assistants
 AI decision-making APIs
```
```
 LLM Abuse for Malicious Capabilities
 Information leakage via prompt chaining
 Hands-on Labs:
 Perform prompt injection on a demo AI chatbot
 Bypass LLM safety layers with adversarial inputs
 Simulate and mitigate jailbreak attempts
```